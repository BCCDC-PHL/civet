import csv
from Bio import SeqIO
import os
import collections

if config["fasta"] != "":
    rule all:
        input:
            os.path.join(config["outdir"],"seqs_of_interest.aln.fasta.treefile"),
            os.path.join(config["outdir"],"lineage_report.md"),
            os.path.join(config["outdir"],"pangolin_seqs_of_interest.csv"),
            os.path.join(config["outdir"],"non_cog.fasta")
else:
    rule all:
        input:
            os.path.join(config["outdir"],"seqs_of_interest.aln.fasta.treefile"),
            os.path.join(config["outdir"],"lineage_report.md"),
            os.path.join(config["outdir"],"pangolin_seqs_of_interest.csv")

rule check_cog_db:
    input:
        query = config["query"],
        cog_seqs = config["cog_seqs"],
        metadata = config["cog_metadata"]
    output:
        cog = os.path.join(config["outdir"],"query_in_cog.csv"),
        cog_seqs = os.path.join(config["outdir"],"query_in_cog.fasta"),
        not_cog = os.path.join(config["outdir"],"not_in_cog.csv")
    run:
        query_names = []
        in_cog_metadata = []
        in_cog_names = set()
        with open(input.query,newline="") as f:
            reader = csv.DictReader(f)
            for row in reader:
                query_names.append(row["name"])

        with open(input.metadata,newline="") as f:
            reader = csv.DictReader(f)
            header_names = reader.fieldnames
            for row in reader:
                for seq in query_names:
                    if seq in row["sequence_name"]:
                        in_cog_metadata.append(row)
                        in_cog_names.add(row["sequence_name"])

            print(f"Number of seqs found in metadata: {len(in_cog_metadata)}")
            with open(output.cog, "w") as fw:
                writer = csv.DictWriter(fw, fieldnames=header_names)
                writer.writeheader()
                writer.writerows(in_cog_metadata)

        fw = open(output.cog_seqs, "w")
        for record in SeqIO.parse(input.cog_seqs, "fasta"):
            if record.id in query_names:
                fw.write(f">{record.id}\n{record.seq}\n")

        with open(output.not_cog, "w") as fw:
            for query in query_seqs:
                if not query in in_cog_names:
                    fw.write(query + '\n')

rule non_cog_minimap2_to_reference:
    input:
        fasta = config["post_qc_query"],
        reference = config["reference_fasta"]
    output:
        sam = os.path.join(config["output_path"],"post_qc_query.reference_mapped.sam")
    shell:
        """
        minimap2 -a -x asm5 {input.reference} {input.fasta} > {output.sam}
        """

rule non_cog_remove_insertions_and_trim_and_pad:
    input:
        sam = rules.non_cog_minimap2_to_reference.output.sam,
        reference = config["reference_fasta"]
    params:
        trim_start = config["trim_start"],
        trim_end = config["trim_end"],
        insertions = os.path.join(config["output_path"],"post_qc_query.insertions.txt")
    output:
        fasta = os.path.join(config["output_path"],"post_qc_query.alignment.trimmed.fasta")
    shell:
        """
        datafunk sam_2_fasta \
          -s {input.sam} \
          -r {input.reference} \
          -o {output.fasta} \
          -t [{params.trim_start}:{params.trim_end}] \
          --pad \
          --log-inserts &
        mv insertions.txt {params.insertions}
        """

# rule pangolin_sequences:
#     input:
#         config["post_qc_query"]
#     threads: workflow.cores
#     params:
#         cores = workflow.cores
#     output:
#         report = os.path.join(config["outdir"],"/pangolin_query_seqs.csv")
#     shell:
#         "pangolin {input[0]:q} -t {params.cores} --outfile {output.report:q}"

rule minimap2_against_cog:
    input:
        config["post_qc_query"],
        cog_seqs = config["cog_seqs"]
    output:
        paf = os.path.join(config["output_path"],"post_qc_query.cog_mapped.paf")
    shell:
        """
        minimap2 -x asm5 --secondary=no --paf-no-hit {input.reference} {input.fasta} > {output.paf}
        """

rule parse_paf:
    input:
        paf = rules.minimap2_against_cog.output.paf,
        metadata = config["cog_metadata"],
        fasta = config["cog_seqs"]
    output:
        fasta = os.path.join(config["output_path"],"closest_cog.fasta"),
        csv = os.path.join(config["output_path"],"closest_cog.csv")
    shell:
        """
        parse_paf.py \
        --paf {input.paf:q} \
        --metadata {input.metadata:q} \
        --seqs {input.fasta} \
        --csv-out {output.csv:q} \
        --seqs-out {output.fasta:q}
        """

rule combine_metadata:
    input:
        closest_cog = rules.parse_paf.output.csv,
        in_cog = rules.check_cog_db.output.cog
    output:
        combined_csv = os.path.join(config["output_path"],"combined_metadata.csv")
    shell:
        

# rule mafft_sequences:
#     input:
#         rules.grab_seqs.output.fasta
#     threads: workflow.cores
#     params:
#         cores = workflow.cores
#     output:
#         fasta = config["outdir"] + "/seqs_of_interest.aln.fasta"
#     shell:
#         "mafft --thread {params.cores} {input[0]:q} > {output[0]:q}"

# rule iqtree_representative_sequences:
#     input:
#         rules.mafft_sequences.output.fasta
#     threads: workflow.cores
#     params:
#         cores = workflow.cores
#     output:
#         config["outdir"] + "/seqs_of_interest.aln.fasta.treefile"
#     shell:
#         "iqtree -s {input[0]:q} -nt AUTO -bb 10000 -m HKY -redo -au -alrt 1000"


